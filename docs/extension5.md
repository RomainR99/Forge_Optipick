# Extension 5 : Apprentissage par Renforcement (RL) pour l'Allocation Optimale

## üìã Vue d'ensemble

L'**Extension 5** utilise l'**Apprentissage par Renforcement (RL)** pour apprendre une strat√©gie d'allocation optimale des commandes aux agents. Contrairement aux approches d√©terministes (MiniZinc, CP-SAT), le RL apprend √† partir de l'exp√©rience et peut s'adapter √† des patterns complexes difficiles √† mod√©liser explicitement.

### Contexte op√©rationnel

Dans un entrep√¥t r√©el :
- Les situations varient constamment (nouvelles commandes, pannes, congestion)
- Des patterns complexes √©mergent (certaines combinaisons commande-agent sont plus efficaces)
- L'environnement √©volue (apprentissage continu)

Le RL permet d'apprendre ces patterns et de s'adapter automatiquement.

---

## üéØ Objectifs de l'extension

1. **Apprendre une politique d'allocation** : Quelle commande assigner √† quel agent dans quelles conditions
2. **Optimiser les r√©compenses** : Maximiser la performance globale (distance, co√ªt, respect des deadlines)
3. **S'adapter dynamiquement** : Apprendre de nouvelles situations sans reprogrammation
4. **Int√©grer avec MiniZinc** : Utiliser la politique apprise pour guider l'optimisation

---

## üß† Concepts du RL

### √âl√©ments fondamentaux

1. **√âtat (State)** : Configuration actuelle de l'entrep√¥t
   - Commandes en attente
   - √âtat des agents (disponibilit√©, charge)
   - Zones congestionn√©es
   - Historique r√©cent

2. **Action (Action)** : D√©cision √† prendre
   - Assigner une commande √† un agent
   - Ordre de visite des emplacements (pour TSP)

3. **R√©compense (Reward)** : Score de performance
   - `-distance` : Minimiser la distance parcourue
   - `-co√ªt` : Minimiser le co√ªt op√©rationnel
   - `+respect_deadline` : Bonus si deadline respect√©e
   - P√©nalit√©s pour les √©checs

4. **Politique (Policy)** : Strat√©gie apprise
   - Fonction qui mappe √©tat ‚Üí action
   - Apprise par essais-erreurs

---

## üîß Architecture propos√©e

### 1Ô∏è‚É£ Environnement RL (Gymnasium/Stable-Baselines3)

```python
import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import Dict, List, Tuple, Optional

class WarehouseAllocationEnv(gym.Env):
    """
    Environnement RL pour l'allocation de commandes dans un entrep√¥t.
    """
    
    def __init__(
        self,
        warehouse: Warehouse,
        agents: List[Agent],
        orders: List[Order],
        products_by_id: Dict[str, Product]
    ):
        super().__init__()
        
        self.warehouse = warehouse
        self.agents = agents
        self.orders = orders
        self.products_by_id = products_by_id
        
        # Espace d'observation (√©tat)
        # Vecteur de features : [commandes_features, agents_features, zones_features]
        n_order_features = 10  # poids, volume, zone, express, etc.
        n_agent_features = 8   # capacit√©, vitesse, type, charge, etc.
        n_zone_features = 5    # congestion par zone
        
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(len(orders) * n_order_features + len(agents) * n_agent_features + n_zone_features,),
            dtype=np.float32
        )
        
        # Espace d'action : (order_idx, agent_idx)
        # Action discr√®te : index dans la matrice order√óagent
        self.action_space = spaces.Discrete(len(orders) * len(agents))
        
        # √âtat initial
        self.reset()
    
    def reset(self, seed=None, options=None):
        """R√©initialise l'environnement."""
        super().reset(seed=seed)
        
        # R√©initialiser les agents
        for agent in self.agents:
            agent.assigned_orders = []
            agent.used_weight = 0.0
            agent.used_volume = 0.0
        
        # Commandes non assign√©es
        self.pending_orders = self.orders.copy()
        self.assigned_orders = []
        
        # M√©triques
        self.total_distance = 0.0
        self.total_cost = 0.0
        self.missed_deadlines = 0
        
        observation = self._get_observation()
        info = {}
        
        return observation, info
    
    def step(self, action: int):
        """
        Ex√©cute une action et retourne (observation, reward, terminated, truncated, info).
        """
        # D√©coder l'action : (order_idx, agent_idx)
        order_idx = action // len(self.agents)
        agent_idx = action % len(self.agents)
        
        # V√©rifier la validit√© de l'action
        if order_idx >= len(self.pending_orders):
            # Action invalide : p√©nalit√©
            reward = -100.0
            terminated = False
            truncated = False
            info = {"invalid_action": True}
            return self._get_observation(), reward, terminated, truncated, info
        
        order = self.pending_orders[order_idx]
        agent = self.agents[agent_idx]
        
        # V√©rifier les contraintes
        valid, reason = self._check_constraints(order, agent)
        
        if not valid:
            # Contrainte viol√©e : p√©nalit√©
            reward = -50.0
            terminated = False
            truncated = False
            info = {"constraint_violated": reason}
            return self._get_observation(), reward, terminated, truncated, info
        
        # Assigner la commande
        agent.assign(order)
        self.pending_orders.remove(order)
        self.assigned_orders.append(order)
        
        # Calculer la r√©compense
        reward = self._calculate_reward(order, agent)
        
        # V√©rifier si termin√© (toutes les commandes assign√©es ou impossibles)
        terminated = len(self.pending_orders) == 0
        truncated = False
        
        # Mettre √† jour les m√©triques
        info = {
            "assigned": True,
            "order_id": order.id,
            "agent_id": agent.id,
            "total_assigned": len(self.assigned_orders)
        }
        
        observation = self._get_observation()
        return observation, reward, terminated, truncated, info
    
    def _get_observation(self) -> np.ndarray:
        """
        Construit le vecteur d'observation (√©tat).
        """
        features = []
        
        # Features des commandes en attente
        for order in self.pending_orders:
            features.extend([
                order.total_weight,
                order.total_volume,
                float(order.priority == "express"),
                float(len(order.items)),
                # Zone principale
                float(self._get_order_zone(order)),
                # Temps restant avant deadline (normalis√©)
                self._get_time_until_deadline(order),
                # Fragile
                float(any(self.products_by_id.get(item.product_id, Product(...)).fragile 
                         for item in order.items)),
                # Poids max item
                max((self.products_by_id.get(item.product_id, Product(...)).weight 
                     for item in order.items), default=0.0),
                # Nombre de locations uniques
                float(len(order.unique_locations)),
                0.0  # Padding
            ])
        
        # Padding pour les commandes manquantes
        max_orders = len(self.orders)
        while len(features) < max_orders * 10:
            features.extend([0.0] * 10)
        
        # Features des agents
        for agent in self.agents:
            features.extend([
                agent.capacity_weight,
                agent.capacity_volume,
                agent.speed,
                agent.cost_per_hour,
                float(agent.type == "robot"),
                float(agent.type == "human"),
                agent.used_weight / agent.capacity_weight if agent.capacity_weight > 0 else 0.0,
                agent.used_volume / agent.capacity_volume if agent.capacity_volume > 0 else 0.0
            ])
        
        # Features des zones (congestion)
        zone_features = [0.0] * 5  # √Ä remplir avec les donn√©es de congestion r√©elles
        features.extend(zone_features)
        
        return np.array(features, dtype=np.float32)
    
    def _check_constraints(self, order: Order, agent: Agent) -> Tuple[bool, str]:
        """V√©rifie si l'assignation respecte les contraintes."""
        # Capacit√©
        if not agent.can_take(order):
            return False, "capacity_exceeded"
        
        # Restrictions des robots (simplifi√©)
        if agent.type == "robot":
            # V√©rifier zones interdites, fragile, etc.
            # (impl√©mentation d√©taill√©e)
            pass
        
        return True, "ok"
    
    def _calculate_reward(self, order: Order, agent: Agent) -> float:
        """
        Calcule la r√©compense pour une assignation.
        R√©compense = -distance -co√ªt +respect_deadline
        """
        reward = 0.0
        
        # Distance estim√©e (simplifi√©e)
        distance = self._estimate_order_distance(order)
        reward -= distance * 0.1  # P√©nalit√© distance
        
        # Co√ªt op√©rationnel
        time_estimate = distance / agent.speed if agent.speed > 0 else 0
        cost = time_estimate * agent.cost_per_hour / 3600
        reward -= cost * 10.0  # P√©nalit√© co√ªt
        
        # Respect des deadlines
        if self._check_deadline(order, time_estimate):
            reward += 100.0  # Bonus deadline respect√©e
        
        # Bonus pour express
        if order.priority == "express":
            reward += 50.0
        
        return reward
    
    def _estimate_order_distance(self, order: Order) -> float:
        """Estime la distance pour pr√©parer une commande."""
        entry = self.warehouse.entry_point
        return sum(entry.manhattan(loc) for loc in order.unique_locations)
    
    def _check_deadline(self, order: Order, time_estimate: float) -> bool:
        """V√©rifie si la deadline peut √™tre respect√©e."""
        # Impl√©mentation simplifi√©e
        return True  # √Ä impl√©menter avec les vraies deadlines
    
    def _get_order_zone(self, order: Order) -> int:
        """Retourne la zone principale de la commande."""
        # Impl√©mentation simplifi√©e
        return 0
    
    def _get_time_until_deadline(self, order: Order) -> float:
        """Retourne le temps restant avant deadline (normalis√©)."""
        # Impl√©mentation simplifi√©e
        return 1.0
```

---

## üöÄ Impl√©mentation avec Stable-Baselines3

### Installation

```bash
pip install stable-baselines3[extra] gymnasium
```

### Entra√Ænement

```python
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback

# Cr√©er l'environnement
env = WarehouseAllocationEnv(warehouse, agents, orders, products_by_id)

# Cr√©er un environnement vectoris√© (pour parall√©lisation)
vec_env = make_vec_env(
    lambda: WarehouseAllocationEnv(warehouse, agents, orders, products_by_id),
    n_envs=4  # 4 environnements en parall√®le
)

# Cr√©er le mod√®le (PPO = Proximal Policy Optimization)
model = PPO(
    "MlpPolicy",
    vec_env,
    verbose=1,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01,
    tensorboard_log="./logs/"
)

# Callback pour √©valuation
eval_callback = EvalCallback(
    env,
    best_model_save_path="./models/best_rl_policy/",
    log_path="./logs/eval/",
    eval_freq=10000,
    deterministic=True,
    render=False
)

# Entra√Æner
model.learn(
    total_timesteps=1_000_000,  # 1 million de steps
    callback=eval_callback,
    progress_bar=True
)

# Sauvegarder le mod√®le
model.save("./models/rl_allocation_policy")
```

---

## üîó Int√©gration avec MiniZinc

### Utilisation de la politique apprise

Une fois la politique RL entra√Æn√©e, on peut l'utiliser pour guider l'optimisation MiniZinc :

```python
def get_rl_preference_scores(
    orders: List[Order],
    agents: List[Agent],
    rl_model: PPO,
    warehouse: Warehouse
) -> List[List[float]]:
    """
    G√©n√®re les scores de pr√©f√©rence RL pour chaque paire (commande, agent).
    """
    scores = []
    
    for order in orders:
        order_scores = []
        for agent in agents:
            # Construire l'√©tat pour cette assignation
            state = build_state_for_assignment(order, agent, warehouse)
            
            # Obtenir la probabilit√© d'action depuis le mod√®le RL
            # (simplifi√© : utiliser la politique d√©terministe)
            action = rl_model.predict(state, deterministic=True)[0]
            
            # Score bas√© sur la probabilit√© ou la valeur Q
            score = get_action_score(rl_model, state, action)
            order_scores.append(score)
        
        scores.append(order_scores)
    
    return scores

# Utiliser dans MiniZinc
rl_scores = get_rl_preference_scores(orders, agents, model, warehouse)
instance["rl_preference_scores"] = rl_scores
```

---

## üìä M√©triques et √©valuation

### M√©triques √† suivre pendant l'entra√Ænement

1. **R√©compense moyenne** : Performance globale
2. **Taux d'assignation** : % de commandes assign√©es
3. **Distance moyenne** : Distance parcourue par commande
4. **Co√ªt moyen** : Co√ªt op√©rationnel par commande
5. **Taux de respect des deadlines** : % de deadlines respect√©es

### Comparaison avec MiniZinc

```python
def compare_rl_vs_minizinc(orders, agents, warehouse):
    """Compare les performances RL vs MiniZinc."""
    
    # Solution RL
    rl_assignment = solve_with_rl(orders, agents, warehouse)
    rl_metrics = evaluate_assignment(rl_assignment, orders, agents, warehouse)
    
    # Solution MiniZinc
    minizinc_assignment = allocate_with_minizinc(orders, agents, products_by_id, warehouse)
    minizinc_metrics = evaluate_assignment(minizinc_assignment, orders, agents, warehouse)
    
    print("Comparaison RL vs MiniZinc:")
    print(f"RL - Distance: {rl_metrics['distance']:.2f}, Co√ªt: {rl_metrics['cost']:.2f}")
    print(f"MiniZinc - Distance: {minizinc_metrics['distance']:.2f}, Co√ªt: {minizinc_metrics['cost']:.2f}")
```

---

## üéì Avantages et limitations

### ‚úÖ Avantages du RL

1. **Adaptabilit√©** : S'adapte √† de nouvelles situations
2. **Patterns complexes** : Apprend des patterns difficiles √† mod√©liser
3. **Apprentissage continu** : Am√©liore avec le temps
4. **Robustesse** : G√®re bien les impr√©vus

### ‚ùå Limitations

1. **Temps d'entra√Ænement** : N√©cessite beaucoup de donn√©es et de temps
2. **Exploration** : Peut prendre de mauvaises d√©cisions pendant l'apprentissage
3. **Interpr√©tabilit√©** : Difficile de comprendre pourquoi une d√©cision est prise
4. **Stabilit√©** : Peut n√©cessiter un r√©entra√Ænement r√©gulier

---

## üìù R√©sum√©

| √âl√©ment | Description |
|---------|-------------|
| **√âtat** | Configuration entrep√¥t + commandes en attente |
| **Action** | Assigner commande √† agent |
| **R√©compense** | `-distance -co√ªt +respect_deadline` |
| **Algorithme** | PPO (Proximal Policy Optimization) |
| **Outil** | Stable-Baselines3 |
| **Int√©gration** | Scores de pr√©f√©rence dans MiniZinc |

---

## üîó R√©f√©rences

- **Mod√®le** : `models/allocation.mzn` (lignes 48-50, 162-165)
- **Documentation Stable-Baselines3** : https://stable-baselines3.readthedocs.io/
- **Gymnasium** : https://gymnasium.farama.org/

---

## üí° Exemple complet d'utilisation

```python
from stable_baselines3 import PPO
from src.models import Warehouse, Agent, Order, Product
from src.rl_env import WarehouseAllocationEnv

# Charger les donn√©es
warehouse = load_warehouse("data/warehouse.json")
agents = load_agents("data/agents.json")
orders = load_orders("data/orders.json")
products_by_id = load_products("data/products.json")

# Cr√©er l'environnement
env = WarehouseAllocationEnv(warehouse, agents, orders, products_by_id)

# Entra√Æner le mod√®le
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=1_000_000)
model.save("models/rl_policy")

# Utiliser la politique
obs, info = env.reset()
done = False
while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated

# √âvaluer les performances
print(f"Commandes assign√©es: {len(env.assigned_orders)}/{len(orders)}")
print(f"Distance totale: {env.total_distance:.2f}")
print(f"Co√ªt total: {env.total_cost:.2f}")
```

---

## üéì Conclusion

L'Extension 5 apporte une approche d'apprentissage par renforcement pour l'allocation optimale :

- ‚úÖ **Apprentissage adaptatif** : S'adapte aux patterns complexes
- ‚úÖ **Optimisation continue** : Am√©liore avec l'exp√©rience
- ‚úÖ **Int√©gration flexible** : Peut guider MiniZinc ou fonctionner ind√©pendamment
- ‚úÖ **Robustesse** : G√®re bien les impr√©vus et situations nouvelles

Le RL compl√®te les approches d√©terministes (MiniZinc, CP-SAT) en apportant de l'adaptabilit√© et de l'apprentissage continu, particuli√®rement utile dans des environnements dynamiques et changeants.
